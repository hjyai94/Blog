---
title: 线性回归-最小二乘法-概率视角
date: 2019-12-28 21:33:23
tags: 机器学习理论推导
categories: 学习
---
$$
\begin{aligned}
&\varepsilon \sim N\left(0, \sigma^{2}\right)\\\\
&y=f(w)+\varepsilon\\\\
&f(w)=w^{\top} x\\\\
&\begin{array}{l}
{y=w^{\top} x+\varepsilon} \\\\
{y | x; w \sim N\left(w^{\top} x, \sigma^{2}\right)}
\end{array}
\end{aligned}
$$


$$\mathbb{L}(w)=\log P\left (Y|X ; w \right)=\log \prod_{i=1}^{N} p\left(y_{i} | x_{i} ; w\right)=\sum_{i=1}^{N} \log P\left(y_{i} | x_{i}\right)$$

\begin{aligned}
&=\sum_{i=1}^{N} \log \frac{1}{\sqrt{2\pi}\sigma}+\log \operatorname{exp}\left\lbrace-\frac{\left(y_{i}-w^{\top} x_{i}\right)^{2}}{2\sigma^{2}}\right\rbrace \\\\
&=\sum_{i=1}^{N} \log \frac{1}{\sqrt{2 \pi} \sigma}-\frac{1}{2\sigma^{2}}\left(y_{i}-w^{\top} x_{i}\right)^{2}
\end{aligned}

\begin{aligned}
\hat{w} &=\arg\max_{w} \mathbb{L}(w) \\\\
&=\arg\max _{w} -\frac{1}{2 \sigma^2} \left(y_i-w^{\top} x_{i}\right)^2 \\\\
&=\arg \min _{w}\left(y_i - w^{\top} x_{i}\right)^2 \Leftrightarrow \text{最小二乘估计}
\end{aligned} 

带有均值为$0$，方差为$\sigma^2$的高斯噪声的最大似然估计等价于最小二乘法。

