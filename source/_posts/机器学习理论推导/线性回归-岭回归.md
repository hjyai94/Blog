---
title: 线性回归-岭回归
date: 2019-12-28 23:29:26
tags: 机器学习理论推导
---
$$\begin{aligned}
&D=\left\lbrace(x_1, y_1),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\rbrace \\\\
&x_{i} \in \mathbb{R}^{p}, \quad y_{i} \in \mathbb{R}, \quad i=1,2, \cdots, N
\end{aligned}$$

$$
X = \begin{pmatrix}
{x_{1}}&{x_{2}}&{\cdots}&{x_{N}}
\end{pmatrix}^T 
= \begin{pmatrix}
{x_{11}}&{x_{12}}&{\cdots}&{x_{1p}}\\\\
{x_{21}}&{x_{22}}&{\cdots}&{x_{2p}}\\\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\\\
{x_{N1}}&{x_{N2}}&{\cdots}&{x_{Np}}\\\\
\end{pmatrix}$$

$$
Y=\left(\begin{array}{c}
{y_{1}} \\\\
{y_{2}} \\\\
{\vdots} \\\\
{y_{N}}
\end{array}\right)_{N \times 1}
$$

岭回归：
### 频率角度
$$
\begin{aligned} J(w) &=\sum_{i=1}^{N}\left\|\left| w^{\top} x_{i}-y_{i}\right\|\right|^{2}+\lambda w^{\top} w \\\\
&=\left(w^{\top} X^{\top}-Y^{\top}\right)(X w-Y)+\lambda w^{\top} w \\\\
&={w^{\top} \left(X^{\top} X  +\lambda I \right)w} -2 w^{\top} X^{\top} Y+Y^{\top} Y
\end{aligned}
$$     

$$\hat{w}= \arg \min_w  J(w)$$
\begin{aligned}
\frac{\partial J(w)}{\partial w}=2\left(X^{\top} X+\lambda I\right) w &-2 X^{\top} Y=0 \\\\
\hat{w}=\left(X^{\top} X+\lambda I\right)^{-1} X^{\top} Y
\end{aligned}

### 贝叶斯角度
