---
title: 批归一化
date: 2020-06-26 15:05:48
tags: 计算机视觉
categories: 学习
---
* 为什么要进行Batch Normalization？
为了解决Internal Covariate Shift，其实就是解决神经网络不同层间的输入的统计特性差异。
* 存在Batch Normalization的情况下，如何进行反向传播？
一种理想的方法是用整个数据的均值和方差代替，但是这种方法受限于数据的大小，而且有时候我们只能一批一批的添加数据。较为实用的方法是，采用moving average，刚开始的权重小，后面的权重大。
* Batch Normalization的好处：
允许较大的学习率，减少训练时间，可以更快收敛。可以缓解梯度消失和梯度爆炸问题，因为对于sigmoid和tanh这类激活函数，每一层的数据都落在0附近时梯度比较大。有对抗过拟合的效果。

# 参考文献
[1] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.